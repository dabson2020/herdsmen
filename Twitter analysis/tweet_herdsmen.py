# -*- coding: utf-8 -*-
"""tweet_herdsmen.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18rBsYugUydtfqfBU1JwfH9AL7nQpFd1n
"""

!pip install tweet-preprocessor

import pandas as pd
import numpy as np
from wordcloud import WordCloud, STOPWORDS
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image



tweets = pd.read_csv('/content/drive/MyDrive/yinka/tweets.csv')

tweets.head()

df = tweets[['id','Tweet','Screen Name']]

df.columns = ['id','tweet','location']

df.info()

"""Using tweet preprocessor"""

import preprocessor as p

import warnings
warnings.filterwarnings('ignore')

import re

def clean_tweet(data):
  text = data['tweet']
  text = p.clean(text)
  text = re.sub(r"^b'", "", text)
  text = re.sub(r"\?|\_|\-|\!|\,|\...|\&|\:|\;|\'s|\(|\)|\.", '', text)
  text = re.sub(r"\\n", '', text)
  text = text.lower()
  return text

df['tweet'] = df.apply(clean_tweet, axis=1)

df.head()

pattern = re.compile(r"^b'")

# apply the regex substitution to the 'text' column
df['location'] = df['location'].apply(lambda x: pattern.sub('', x))
df['location'] = df['location'].replace("'",'Nigeria')

df.head()

df = df[['id','tweet']]

df.head()

import nltk

nltk.download('stopwords')

from nltk.corpus import stopwords
import matplotlib.pyplot as plt
import seaborn as sns

stop_words = set(stopwords.words('english'))

df['tweet'] = df['tweet'].apply(lambda x: ' '.join([w for w in x.split() if w not in stop_words]))
df['tweet'] =df['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)> 4]))

df.tweet.head()

all_words = ' '.join([w for w in df['tweet']])

len(all_words)

from wordcloud import WordCloud

xwords = pd.Series(['xe2','x80','xa6','xf0' ,'x9f','x99','x99t','xa3','xa4','xc2'])

banned_words = set(word.lower() for word in xwords)

def delete_banned_words(matchobj):
    word = matchobj.group(0)
    if word.lower() in banned_words:
        return ""
    else:
        return word

word_pattern = re.compile('\w+')

df['tweet'] = [word_pattern.sub(delete_banned_words,sentence) for sentence in df['tweet']]

wordcloud = WordCloud(width = 800, height = 500, random_state = 21, max_font_size = 110,background_color = 'black').generate(all_words)

def wordcloud_draw(data, color = 'black'):
    words = ' '.join(data)
    cleaned_word = " ".join([word for word in words.split()
                            if 'http' not in word
                                and not word.startswith('@')
                                and not word.startswith('#')
                                and word != 'RT'
                            ])
   
    wordcloud = WordCloud(
                      background_color=color,
                          width=2500,
                      height=2000
                     ).generate(cleaned_word)
    plt.figure(1,figsize=(13, 13))
    plt.imshow(wordcloud)
    plt.axis('off')
    plt.show()

"""Let us check the frequecies of word in the tweets"""

words = ' '.join(df['tweet'])

"""tokenize tweet"""

nltk.download('punkt')

from nltk.tokenize import word_tokenize

# tokenize the words
tokens = word_tokenize(words)

tokens

unwanted_words = ['xe7x8fxe5x88xa9xe5xa7xac','xc2xa1nfidel','xc2xa1dxc2xa10t','x98x94x98x94',"'",'x94','b','b',"''",'','xa3i',"'m",'x98x81',"n't",'x87xacx87xa7','esn','\\\\\\', 'foreve\\\\\\', '\\xc2\\xa1d\\xc2\\xa10t','\\xc2\\xa1nfidel','th\\\\\\','m','\\\\\\\\xa3I','\\xe7\\\\x8f\\xe5\\x88\\xa9\\xe5\\xa7\\xac','\\\\\\x98\\x94\\\\\\x98\\x94']

# filter out unwanted words from the list of tokens using list comprehension
filtered_tokens = [word for word in tokens if word not in unwanted_words]

# print the filtered tokens
print(filtered_tokens)

clean_tokens = [word.replace('\\', '') for word in tokens]

clean_tokens

unwanted_words_new = ['xe7x8fxe5x88xa9xe5xa7xac','xc2xa1nfidel','xc2xa1dxc2xa10t','x98x94x98x94',"'",'x94','b','b',"''",'','xa3i',"'m",'x98x81',"n't",'x87xacx87xa7','esn','``','mo'," '","''",'ca',"n't",'th']

# filter out unwanted words from the list of tokens using list comprehension
filtered_tokens = [word for word in clean_tokens if word not in unwanted_words_new]

# print the filtered tokens
print(filtered_tokens)

filtered_tokens = [word.lower() for word in filtered_tokens]

filtered_tokens

from nltk.probability import FreqDist

# create a frequency distribution of the tokens
freq_dist = FreqDist(filtered_tokens)

# create a dataframe from the frequency distribution
freq_df = pd.DataFrame(list(freq_dist.items()), columns=['Word', 'Frequency'])

# sort the dataframe by frequency in descending order
freq_df = freq_df.sort_values(by='Frequency', ascending=False)

freq_df

import matplotlib.pyplot as plt

# create a bar plot of the top 20 words by frequency
plt.bar(freq_df['Word'][:20], freq_df['Frequency'][:20])
plt.xticks(rotation=90)
plt.xlabel('Word')
plt.ylabel('Frequency')
plt.show()

wordcloud_draw(freq_df['Word'],color = 'white')

"""Bigram of words and their frequencies"""

from collections import Counter

"""Plot the bigrams and the frequencies"""

df['tokens'] = df['tweet'].apply(nltk.word_tokenize)

df['bigrams'] = df['tokens'].apply(lambda x: list(nltk.bigrams(x)))

df.head()

bigrams_list = [item for sublist in df['bigrams'] for item in sublist]
bigrams_freq = Counter(bigrams_list)

import re

unwanted_words_new = ['xe7x8fxe5x88xa9xe5xa7xac','xc2xa1nfidel','xc2xa1dxc2xa10t','x98x94x98x94',"'",'\\','x94','b','b',"''",'','xa3i',"'m",'x98x81',"n't",'x87xacx87xa7','esn','``','mo'," '","''",'ca',"n't",'th','xe7x8fxe5x88xa9xe5xa7xac','xc2xa1nfidel','xc2xa1dxc2xa10t','x98x94x98x94',"'",'x94','b','b',"''",'','xa3i',"'m",'x98x81',"n't",'x87xacx87xa7','esn','\\\\\\', 'foreve\\\\\\', '\\xc2\\xa1d\\xc2\\xa10t','\\xc2\\xa1nfidel','th\\\\\\','m','\\\\\\\\xa3I','\\xe7\\\\x8f\\xe5\\x88\\xa9\\xe5\\xa7\\xac','\\\\\\x98\\x94\\\\\\x98\\x94','\\\\\\x98\\x85\\\\\\x98\\x85\\\\\\x98\\x85']
# filter out unwanted words from the list of tokens using list comprehension
filtered_tokens = [word for word in bigrams_list if word not in unwanted_words_new]
filtered_tokens = [(t[0].replace('\\', ''), t[1].replace('\\', '')) for t in filtered_tokens]


# Count the frequency of each bigram
bigram_freq = Counter(filtered_tokens)

# Convert the counter object to a list of tuples
bigram_freq_list = list(bigram_freq.items())

# Sort the list by frequency in descending order
bigram_freq_list = sorted(bigram_freq_list, key=lambda x: x[1], reverse=True)

# Print the top 20 most frequent bigrams and their frequencies
for bigram, freq in bigram_freq_list[:99]:
    print(f"{bigram}: {freq}")

top_30 = bigram_freq_list[:30]

top_30

"""Remove unwanted bigrams"""

my_list = [t for t in top_30 if t != (('herdsmen', "'"), 78) and t !=(('b', "''"), 49) and t != (('ca', "n't"), 40)]

my_list

"""Plot the top_30

convert my_list to a dataframe
"""

top_30_dict = dict(my_list)

df_top_30 = pd.DataFrame(list(top_30_dict.items()), columns=['bigram', 'frequency'])

# Sort the dataframe by frequency in descending order
df_top_30 = df_top_30.sort_values('frequency', ascending=False)

# Plot a barchart of the top 10 bigrams
fig, ax = plt.subplots(figsize=(8, 6))
df_top_30.head(27).plot(kind='bar', x='bigram', y='frequency', ax=ax, color='blue')
plt.title('Most frequent Bigrams')
plt.xlabel('Bigrams')
plt.ylabel('Frequency')
plt.xticks(rotation=90)
plt.show()

"""Plotting the wordcloud"""

top_30_dict

# Create a dictionary of bigrams and their frequencies
bigram_dict = {}
for bigram, freq in my_list:
    bigram_dict[' '.join(bigram)] = freq

# load an image to use as a mask for the wordcloud
mask = np.array(Image.open("/content/drive/MyDrive/yinka/map of nig.png"))


# create a WordCloud object
wc = WordCloud(background_color="white", max_words=2000, mask=mask,contour_width=3, contour_color='steelblue')

# generate the word cloud from your data
wc.generate_from_frequencies(bigram_dict)

# plot the wordcloud
plt.figure(figsize=(6,6))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.show()

"""Topic modelling"""

import gensim
from gensim import corpora
from gensim.models.ldamodel import LdaModel
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

# assuming your dataframe is called "df" and the text column is called "text"
corpus = df['tweet'].values.tolist()

# create count vectorizer object
cv = CountVectorizer(stop_words='english')
cv_fit = cv.fit_transform(corpus)

# create LDA object
lda = LatentDirichletAllocation(n_components=10, random_state=42)
lda.fit(cv_fit)

# get topics and their top 10 words
feature_names = cv.get_feature_names_out()
for idx, topic in enumerate(lda.components_):
    print("Topic #%d:" % idx)
    print(" ".join([feature_names[i] for i in topic.argsort()[:-11:-1]]))

